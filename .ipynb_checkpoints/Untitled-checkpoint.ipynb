{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data chunks and converts to numpy arrays\n",
    "def load_pose_data():\n",
    "    pose = np.array(loadmat('./data/pose.mat')['pose'])\n",
    "    illum = np.array(loadmat('./data/illumination.mat')['illum'])\n",
    "    return pose, illum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose, illum = load_pose_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds(pose):\n",
    "    pose_X = []\n",
    "    pose_y = []\n",
    "    for subject in range(68):\n",
    "        for img in range(13):\n",
    "            pose_X.append(pose[:,:,img,subject].reshape((48*40)))\n",
    "            pose_y.append(subject)\n",
    "    pose_X = np.array(pose_X)\n",
    "    pose_y = np.transpose(np.array(pose_y))\n",
    "    return pose_X, pose_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "posex, posey = ds(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 1920)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pose_dataset(pose, num_subjects, test_size=60):\n",
    "    pose_dataset = []\n",
    "    pose_labels = []\n",
    "    if augmented == False:\n",
    "        for subject in range(num_subjects):\n",
    "            for pose in range(13):\n",
    "                pose_dataset.append(pose[0][:,:,pose,subject].reshape((48*40)))\n",
    "                pose_labels.append(subject)\n",
    "    else:\n",
    "        for subject in range(num_subjects):\n",
    "            for pose in range(13):\n",
    "                pose_dataset.append(raw_pose[0][:,:,pose,subject].reshape((48*40)))\n",
    "                pose_labels.append(subject)\n",
    "        for subject in range(num_subjects):\n",
    "            for pose in range(21):\n",
    "                img = raw_pose[1][:,pose,subject].reshape((40,48))\n",
    "                img = np.flip(np.rot90(img)).reshape((48*40))\n",
    "                pose_dataset.append(img)\n",
    "                pose_labels.append(subject)\n",
    "                \n",
    "    pose_dataset = np.array(pose_dataset)\n",
    "    pose_labels = np.transpose(np.array(pose_labels))\n",
    "    return pose_dataset, pose_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view mnist data\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-58bf197965ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#SIMPLE SEQUENTIAL MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flatten 28*28 images to a 784 vector for each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#SIMPLE SEQUENTIAL MODEL\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.2339 - accuracy: 0.9325 - val_loss: 0.0778 - val_accuracy: 0.9763\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0706 - accuracy: 0.9790 - val_loss: 0.0463 - val_accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0501 - accuracy: 0.9848 - val_loss: 0.0466 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 0.0414 - val_accuracy: 0.9863\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.0337 - val_accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 8s 28ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0371 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 8s 28ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0367 - val_accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.0301 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.0336 - val_accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 8s 27ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.0369 - val_accuracy: 0.9880\n",
      "CNN Error: 1.20%\n"
     ]
    }
   ],
   "source": [
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define a simple CNN model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.3737 - accuracy: 0.8841 - val_loss: 0.0930 - val_accuracy: 0.9690\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 13s 45ms/step - loss: 0.0969 - accuracy: 0.9699 - val_loss: 0.0496 - val_accuracy: 0.9841\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.0692 - accuracy: 0.9784 - val_loss: 0.0402 - val_accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.0557 - accuracy: 0.9825 - val_loss: 0.0343 - val_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.0472 - accuracy: 0.9849 - val_loss: 0.0325 - val_accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.0410 - accuracy: 0.9874 - val_loss: 0.0354 - val_accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 14s 48ms/step - loss: 0.0388 - accuracy: 0.9875 - val_loss: 0.0271 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 15s 50ms/step - loss: 0.0334 - accuracy: 0.9893 - val_loss: 0.0290 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 15s 49ms/step - loss: 0.0309 - accuracy: 0.9897 - val_loss: 0.0256 - val_accuracy: 0.9906\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.0276 - accuracy: 0.9907 - val_loss: 0.0238 - val_accuracy: 0.9906\n",
      "Large CNN Error: 0.94%\n"
     ]
    }
   ],
   "source": [
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
